{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import tqdm\n",
    "import math\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans,  MiniBatchKMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import auc, accuracy_score, mean_squared_error, pairwise_distances_argmin_min, confusion_matrix, homogeneity_score\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import tree\n",
    "\n",
    "from new_jmlm import JMLM\n",
    "from load_data import load_data\n",
    "from isodata import isodata_classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: mnist\n",
      "(60000, 784)\n"
     ]
    }
   ],
   "source": [
    "dataset = \"mnist\"\n",
    "X_train, X_test, y_train, y_test = load_data(dataset)\n",
    "\n",
    "print(f\"Dataset: {dataset}\")\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_train)\n",
    "acc = accuracy_score(y_train, y_pred)\n",
    "print(f'Prediction Accuracy: {acc}')\n",
    "y_pred = clf.predict(X_test)    \n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f'Prediction Accuracy: {acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.20)\n",
    "clf = MLPClassifier(hidden_layer_sizes=(10,  )).fit(X_train, y_train)\n",
    "\n",
    "print(clf.score(X_valid, y_valid))\n",
    "\n",
    "print(clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_cluster_labels(kmeans, actual_labels):\n",
    "    \"\"\"\n",
    "    Associates most probable label with each cluster in KMeans model\n",
    "    returns: dictionary of clusters assigned to each label\n",
    "    \"\"\"\n",
    "\n",
    "    inferred_labels = {}\n",
    "\n",
    "    # Loop through the clusters\n",
    "    for i in range(kmeans.n_clusters):\n",
    "\n",
    "        # find index of points in cluster\n",
    "        labels = []\n",
    "        index = np.where(kmeans.labels_ == i)\n",
    "\n",
    "        # append actual labels for each point in cluster\n",
    "        labels.append(actual_labels[index])\n",
    "\n",
    "        # determine most common label\n",
    "        if len(labels[0]) == 1:\n",
    "            counts = np.bincount(labels[0])\n",
    "        else:\n",
    "            counts = np.bincount(np.squeeze(labels))\n",
    "\n",
    "        # assign the cluster to a value in the inferred_labels dictionary\n",
    "        if np.argmax(counts) in inferred_labels:\n",
    "            # append the new number to the existing array at this slot\n",
    "            inferred_labels[np.argmax(counts)].append(i)\n",
    "        else:\n",
    "            # create a new array in this slot\n",
    "            inferred_labels[np.argmax(counts)] = [i]\n",
    "        \n",
    "    return inferred_labels  \n",
    "\n",
    "def infer_data_labels(X_labels, cluster_labels):\n",
    "    \"\"\"\n",
    "    Determines label for each array, depending on the cluster it has been assigned to.\n",
    "    returns: predicted labels for each array\n",
    "    \"\"\"\n",
    "    \n",
    "    # empty array of len(X)\n",
    "    predicted_labels = np.zeros(len(X_labels)).astype(np.uint8)\n",
    "    \n",
    "    for i, cluster in enumerate(X_labels):\n",
    "        for key, value in cluster_labels.items():\n",
    "            if cluster in value:\n",
    "                predicted_labels[i] = key\n",
    "                \n",
    "    return predicted_labels\n",
    "\n",
    "def calc_metrics(estimator, data, labels):\n",
    "    print('Number of Clusters: {}'.format(estimator.n_clusters))\n",
    "    # Inertia\n",
    "    inertia = estimator.inertia_\n",
    "    print(\"Inertia: {}\".format(inertia))\n",
    "    # Homogeneity Score\n",
    "    homogeneity = homogeneity_score(labels, estimator.labels_)\n",
    "    print(\"Homogeneity score: {}\".format(homogeneity))\n",
    "    return inertia, homogeneity\n",
    "\n",
    "def low_dim(X_train, X_test, y_train, n_pca=0, pca=True, lda=True):\n",
    "    if pca:\n",
    "        if n_pca == 0:\n",
    "            n_pca = int(X_train.shape[1]*0.8)\n",
    "        pca = PCA(n_components=n_pca)\n",
    "        X_train = pca.fit_transform(X_train)\n",
    "        X_test = pca.transform(X_test)\n",
    "    if lda:\n",
    "        clf = LinearDiscriminantAnalysis()\n",
    "        X_train = clf.fit_transform(X_train, y_train.argmax(axis=1))\n",
    "        X_test = clf.transform(X_test)\n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = load_data(dataset)\n",
    "\n",
    "X_train, X_test = low_dim(X_train, X_test, y_train)\n",
    "\n",
    "n_digits = len(np.unique(y_train))\n",
    "\n",
    "kmeans = KMeans(n_clusters=256).fit(X_train)\n",
    "\n",
    "cluster_labels = infer_cluster_labels(kmeans, y_train.argmax(axis=1))\n",
    "X_train_clusters = kmeans.predict(X_train)\n",
    "y_train_pred = infer_data_labels(X_train_clusters, cluster_labels)\n",
    "acc_train = accuracy_score(y_train.argmax(axis=1), y_train_pred)\n",
    "print(f'Train Prediction Accuracy: {acc_train}')\n",
    "\n",
    "X_test_clusters = kmeans.predict(X_test)\n",
    "y_test_pred = infer_data_labels(X_test_clusters, cluster_labels)\n",
    "acc_test = accuracy_score(y_test.argmax(axis=1), y_test_pred)\n",
    "print(f'Test Prediction Accuracy: {acc_test}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = load_data(dataset, onehot=False)\n",
    "clusters = [10, 16, 36, 64, 144, 256]\n",
    "iner_list = []\n",
    "homo_list = []\n",
    "acc_list = []\n",
    "\n",
    "for n_clusters in clusters:\n",
    "    estimator = MiniBatchKMeans(n_clusters=n_clusters)\n",
    "    estimator.fit(X_train)\n",
    "    \n",
    "    inertia, homo = calc_metrics(estimator, X_train, y_train)\n",
    "    iner_list.append(inertia)\n",
    "    homo_list.append(homo)\n",
    "    \n",
    "    # Determine predicted labels\n",
    "    cluster_labels = infer_cluster_labels(estimator, y_train)\n",
    "    prediction = infer_data_labels(estimator.labels_, cluster_labels)\n",
    "    \n",
    "    acc = accuracy_score(y_train, prediction)\n",
    "    acc_list.append(acc)\n",
    "    print('Accuracy: {}\\n'.format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(16, 10))\n",
    "ax[0].plot(clusters, iner_list, label='inertia', marker='o')\n",
    "ax[1].plot(clusters, homo_list, label='homogeneity', marker='o')\n",
    "ax[1].plot(clusters, acc_list, label='accuracy', marker='^')\n",
    "ax[0].legend(loc='best')\n",
    "ax[1].legend(loc='best')\n",
    "ax[0].grid('on')\n",
    "ax[1].grid('on')\n",
    "ax[0].set_title('Inertia of each clusters')\n",
    "ax[1].set_title('Homogeneity and Accuracy of each clusters')\n",
    "fig.suptitle('matplotlib.axes.Axes.set_title() function Example\\n')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(max_depth=500)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_Train_pred = clf.predict(X_train)\n",
    "\n",
    "acc_train = accuracy_score(y_train, y_Train_pred)\n",
    "\n",
    "print(f'Prediction Accuracy: {acc_train}')\n",
    "\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f'Prediction Accuracy: {acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = load_data(dataset, onehot=False)\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(objective=\"binary:logistic\")\n",
    "# xgb_model = xgb.XGBClassifier(objective=\"multi:softprob\", random_state=42)\n",
    "\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = xgb_model.predict(X_train)\n",
    "\n",
    "print(confusion_matrix(y_train, y_pred))\n",
    "acc = accuracy_score(y_train, y_pred)\n",
    "print(f'Prediction Accuracy: {acc}')\n",
    "\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f'Prediction Accuracy: {acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "dataset = \"mnist\"\n",
    "X_train, X_test, y_train, y_test = load_data(dataset, onehot=False)\n",
    "X = np.vstack((X_train, X_test)) / 255.0\n",
    "y = np.append(y_train, y_test)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "feat_cols = [ 'pixel' + str(i) for i in range(X.shape[1]) ]\n",
    "\n",
    "df = pd.DataFrame(X,columns=feat_cols)\n",
    "df['y'] = y\n",
    "df['label'] = df['y'].apply(lambda i: str(i))\n",
    "\n",
    "print('Size of the dataframe: {}'.format(df.shape))\n",
    "\n",
    "\n",
    "rndperm = np.random.permutation(df.shape[0])\n",
    "\n",
    "plt.gray()\n",
    "\n",
    "fig = plt.figure( figsize=(16,7) )\n",
    "\n",
    "for i in range(0, 5):\n",
    "    ax = fig.add_subplot(1, 5, i+1, title=\"Digit: {}\".format(str(df.loc[rndperm[i], 'label'])))\n",
    "\n",
    "    ax.matshow(df.loc[rndperm[i],feat_cols].values.reshape((28,28)).astype(float))\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=3)\n",
    "pca_result = pca.fit_transform(df[feat_cols].values)\n",
    "\n",
    "df['pca-one'] = pca_result[:,0]\n",
    "df['pca-two'] = pca_result[:,1] \n",
    "df['pca-three'] = pca_result[:,2]\n",
    "\n",
    "print('Explained variation per principal component: {}'.format(pca.explained_variance_ratio_))\n",
    "\n",
    "plt.figure(figsize=(16,10))\n",
    "\n",
    "sns.scatterplot(\n",
    "    x=\"pca-one\", \n",
    "    y=\"pca-two\",\n",
    "    hue=\"y\",\n",
    "    palette=sns.color_palette(\"hls\", 10),\n",
    "    data=df.loc[rndperm,:],\n",
    "    legend=\"full\",\n",
    "    alpha=0.3\n",
    ")\n",
    "\n",
    "\n",
    "ax = plt.figure(figsize=(16,10)).gca(projection='3d')\n",
    "ax.scatter(\n",
    "    xs=df.loc[rndperm,:][\"pca-one\"], \n",
    "    ys=df.loc[rndperm,:][\"pca-two\"], \n",
    "    zs=df.loc[rndperm,:][\"pca-three\"], \n",
    "    c=df.loc[rndperm,:][\"y\"], \n",
    "    cmap='tab10'\n",
    ")\n",
    "ax.set_xlabel('pca-one')\n",
    "ax.set_ylabel('pca-two')\n",
    "ax.set_zlabel('pca-three')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10000\n",
    "\n",
    "df_subset = df.loc[rndperm[:N],:].copy()\n",
    "\n",
    "data_subset = df_subset[feat_cols].values\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "pca_result = pca.fit_transform(data_subset)\n",
    "\n",
    "df_subset['pca-one'] = pca_result[:,0]\n",
    "df_subset['pca-two'] = pca_result[:,1] \n",
    "df_subset['pca-three'] = pca_result[:,2]\n",
    "\n",
    "print('Explained variation per principal component: {}'.format(pca.explained_variance_ratio_))\n",
    "\n",
    "time_start = time.time()\n",
    "tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)\n",
    "tsne_results = tsne.fit_transform(data_subset)\n",
    "\n",
    "print('t-SNE done! Time elapsed: {} seconds'.format(time.time()-time_start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_subset['tsne-2d-one'] = tsne_results[:,0]\n",
    "df_subset['tsne-2d-two'] = tsne_results[:,1]\n",
    "\n",
    "plt.figure(figsize=(16,10))\n",
    "sns.scatterplot(\n",
    "    x=\"tsne-2d-one\", y=\"tsne-2d-two\",\n",
    "    hue=\"y\",\n",
    "    palette=sns.color_palette(\"hls\", 10),\n",
    "    data=df_subset,\n",
    "    legend=\"full\",\n",
    "    alpha=0.3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(\"D:/Applications/vscode/workspace/JMLM/datasets/ICU/ICU_ALL_DATA_test.txt\", sep=\"\t\", header=None) \n",
    "train_data = pd.read_csv(\"D:/Applications/vscode/workspace/JMLM/datasets/ICU/ICU_ALL_DATA_train.txt\", sep=\"\t\", header=None) \n",
    "# data = train_data.append(test_data, ignore_index=True)\n",
    "data = [train_data, test_data]\n",
    "data = pd.concat(data)\n",
    "X = data.iloc[:, 1:10].to_numpy()\n",
    "y = data.iloc[:, 0].to_numpy()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
    "print(X_train.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import igraph\n",
    "from igraph import Graph, EdgeSeq\n",
    "nr_vertices = 25\n",
    "v_label = list(map(str, range(nr_vertices)))\n",
    "G = Graph.Tree(nr_vertices, 2) # 2 stands for children number\n",
    "lay = G.layout('rt')\n",
    "\n",
    "position = {k: lay[k] for k in range(nr_vertices)}\n",
    "Y = [lay[k][1] for k in range(nr_vertices)]\n",
    "M = max(Y)\n",
    "\n",
    "es = EdgeSeq(G) # sequence of edges\n",
    "E = [e.tuple for e in G.es] # list of edges\n",
    "\n",
    "L = len(position)\n",
    "Xn = [position[k][0] for k in range(L)]\n",
    "Yn = [2*M-position[k][1] for k in range(L)]\n",
    "Xe = []\n",
    "Ye = []\n",
    "for edge in E:\n",
    "    Xe+=[position[edge[0]][0],position[edge[1]][0], None]\n",
    "    Ye+=[2*M-position[edge[0]][1],2*M-position[edge[1]][1], None]\n",
    "\n",
    "labels = v_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=Xe,\n",
    "                   y=Ye,\n",
    "                   mode='lines',\n",
    "                   line=dict(color='rgb(210,210,210)', width=1),\n",
    "                   hoverinfo='none'\n",
    "                   ))\n",
    "fig.add_trace(go.Scatter(x=Xn,\n",
    "                  y=Yn,\n",
    "                  mode='markers',\n",
    "                  name='bla',\n",
    "                  marker=dict(symbol='circle-dot',\n",
    "                                size=18,\n",
    "                                color='#6175c1',    #'#DB4551',\n",
    "                                line=dict(color='rgb(50,50,50)', width=1)\n",
    "                                ),\n",
    "                  text=labels,\n",
    "                  hoverinfo='text',\n",
    "                  opacity=0.8\n",
    "                  ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_annotations(pos, text, font_size=10, font_color='rgb(250,250,250)'):\n",
    "    L=len(pos)\n",
    "    if len(text)!=L:\n",
    "        raise ValueError('The lists pos and text must have the same len')\n",
    "    annotations = []\n",
    "    for k in range(L):\n",
    "        annotations.append(\n",
    "            dict(\n",
    "                text=labels[k], # or replace labels with a different list for the text within the circle\n",
    "                x=pos[k][0], y=2*M-position[k][1],\n",
    "                xref='x1', yref='y1',\n",
    "                font=dict(color=font_color, size=font_size),\n",
    "                showarrow=False)\n",
    "        )\n",
    "    return annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axis = dict(showline=False, # hide axis line, grid, ticklabels and  title\n",
    "            zeroline=False,\n",
    "            showgrid=False,\n",
    "            showticklabels=False,\n",
    "            )\n",
    "\n",
    "fig.update_layout(title= 'Tree with Reingold-Tilford Layout',\n",
    "              annotations=make_annotations(position, v_label),\n",
    "              font_size=12,\n",
    "              showlegend=False,\n",
    "              xaxis=axis,\n",
    "              yaxis=axis,\n",
    "              margin=dict(l=40, r=40, b=85, t=100),\n",
    "              hovermode='closest',\n",
    "              plot_bgcolor='rgb(248,248,248)'\n",
    "              )\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import igraph as ig\n",
    "from igraph import Graph\n",
    "\n",
    "# Set seed for reproducibility\n",
    "import random\n",
    "random.seed(123)\n",
    "\n",
    "# Create an undirected tree. If your tree is not undirected, \n",
    "# convert it to undirected first.\n",
    "g = Graph.Tree_Game(10)\n",
    "\n",
    "# Our chosen root:\n",
    "root = 3\n",
    "\n",
    "ig.plot(g, vertex_label=range(g.vcount()), layout = g.layout_reingold_tilford(root=root), bbox=(300,300))\n",
    "\n",
    "# Distances from the root, will be used for ordering:\n",
    "dist=g.shortest_paths(source=root)[0]\n",
    "\n",
    "# This function computes the permutation that would\n",
    "# sort 'elems'. It also serves as a way to invert\n",
    "# permutations.\n",
    "\n",
    "def ordering(elems):\n",
    "    return sorted(range(len(elems)), key=elems.__getitem__)\n",
    "\n",
    "# Compute orderings based on the distance from the root:\n",
    "perm = ordering(dist)\n",
    "invperm = ordering(perm)\n",
    "\n",
    "# Reorder, direct, restore order:\n",
    "dg = g.permute_vertices(invperm)\n",
    "dg.to_directed('acyclic')\n",
    "dg = dg.permute_vertices(perm)\n",
    "\n",
    "# Plot again.\n",
    "# Now the root does not need to be given,\n",
    "# as it is auto-detected from the directions.\n",
    "ig.plot(dg, vertex_label=range(g.vcount()), layout='reingold_tilford', bbox=(300,300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import igraph as ig\n",
    "g = ig.Graph(n = 12, directed=True)\n",
    "g.add_edges([(1,0),(2,1), (3,2), (4,3),\n",
    "         (5,1),\n",
    "         (6,2), (7,6), (8,7),\n",
    "         (9,0),\n",
    "         (10,0), (11,10)])\n",
    "g.vs[\"label\"] = [\"A\", \"B\", \"A\", \"B\", \"C\", \"F\", \"C\", \"B\", \"D\", \"C\", \"D\", \"F\"]\n",
    "layout = g.layout_reingold_tilford(mode=\"in\", root=[0])\n",
    "ig.plot(g, layout=layout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from graphviz import Digraph\n",
    "\n",
    "os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files/Graphviz/bin'\n",
    "\n",
    "dot = Digraph(comment='The Round Table')\n",
    "names = ['剪刀', '石頭', '布']\n",
    "\n",
    "for i in names:  #新增三個結點，分別叫做剪刀石頭布\n",
    "    dot.node(i, i)\n",
    "\n",
    "\n",
    "for i in range(len(names)): #將互相克制的關係畫上去\n",
    "    dot.edge(names[i], names[i-1], \"克制\")\n",
    "\n",
    "dot.render('./round-table.png', view=True)\n",
    "\n",
    "print(str(dot))\n",
    "\n",
    "dot\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from graphviz import Digraph\n",
    "\n",
    "os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files/Graphviz/bin'\n",
    "\n",
    "dot = Digraph(comment='Tree')\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    dot.node(str(i), str(i))\n",
    "    dot.edge(str(i), str(i), \"s\")\n",
    "\n",
    "\n",
    "dot.view()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def add_gaussian_noise(X_imgs):\n",
    "    gaussian_noise_imgs = []\n",
    "    row, col, _ = X_imgs[0].shape\n",
    "    # Gaussian distribution parameters\n",
    "    mean = 0\n",
    "    var = 0.1\n",
    "    sigma = var ** 0.5\n",
    "    \n",
    "    for X_img in X_imgs:\n",
    "        gaussian = np.random.random((row, col, 1)).astype(np.float32)\n",
    "        gaussian = np.concatenate((gaussian, gaussian, gaussian), axis = 2)\n",
    "        gaussian_img = cv2.addWeighted(X_img, 0.75, 0.25 * gaussian, 0.25, 0)\n",
    "        gaussian_noise_imgs.append(gaussian_img)\n",
    "    gaussian_noise_imgs = np.array(gaussian_noise_imgs, dtype = np.float32)\n",
    "    return gaussian_noise_imgs\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9866666666666667"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "X, y = load_iris(return_X_y=True)\n",
    "kernel = 1.0 * RBF(1.0)\n",
    "gpc = GaussianProcessClassifier(kernel=kernel, random_state=0).fit(X, y)\n",
    "gpc.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 2])\n",
      "Parameter containing:\n",
      "tensor([[0.2500, 0.7500],\n",
      "        [0.7500, 0.7500],\n",
      "        [0.2500, 0.5000],\n",
      "        [0.5000, 0.5000],\n",
      "        [0.7500, 0.5000],\n",
      "        [0.2500, 0.2500],\n",
      "        [0.7500, 0.2500],\n",
      "        [0.5000, 0.1250]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch, random\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    " \n",
    "torch.manual_seed(42)\n",
    " \n",
    "class RBFN(nn.Module):\n",
    "    \"\"\"\n",
    "    以高斯核作为径向基函数\n",
    "    \"\"\"\n",
    "    def __init__(self, centers, n_out=3):\n",
    "        \"\"\"\n",
    "        :param centers: shape=[center_num,data_dim]\n",
    "        :param n_out:\n",
    "        \"\"\"\n",
    "        super(RBFN, self).__init__()\n",
    "        self.n_out = n_out\n",
    "        self.num_centers = centers.size(0) # 隐层节点的个数\n",
    "        self.dim_centure = centers.size(1) # \n",
    "        self.centers = nn.Parameter(centers)\n",
    "        # self.beta = nn.Parameter(torch.ones(1, self.num_centers), requires_grad=True)\n",
    "        self.beta = torch.ones(1, self.num_centers)*10\n",
    "        # 对线性层的输入节点数目进行了修改\n",
    "        self.linear = nn.Linear(self.num_centers+self.dim_centure, self.n_out, bias=True)\n",
    "        self.initialize_weights()# 创建对象时自动执行\n",
    "        print(self.centers)\n",
    " \n",
    " \n",
    "    def kernel_fun(self, batches):\n",
    "        n_input = batches.size(0)  # number of inputs\n",
    "        A = self.centers.view(self.num_centers, -1).repeat(n_input, 1, 1)\n",
    "        B = batches.view(n_input, -1).unsqueeze(1).repeat(1, self.num_centers, 1)\n",
    "        C = torch.exp(-self.beta.mul((A - B).pow(2).sum(2, keepdim=False)))\n",
    "        return C\n",
    " \n",
    "    def forward(self, batches):\n",
    "        radial_val = self.kernel_fun(batches)\n",
    "        class_score = self.linear(torch.cat([batches, radial_val], dim=1))\n",
    "        return class_score\n",
    " \n",
    "    def initialize_weights(self, ):\n",
    "        \"\"\"\n",
    "        网络权重初始化\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                m.weight.data.normal_(0, 0.02)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.ConvTranspose2d):\n",
    "                m.weight.data.normal_(0, 0.02)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                m.weight.data.normal_(0, 0.02)\n",
    "                m.bias.data.zero_()\n",
    " \n",
    "    def print_network(self):\n",
    "        num_params = 0\n",
    "        for param in self.parameters():\n",
    "            num_params += param.numel()\n",
    "        print(self)\n",
    "        print('Total number of parameters: %d' % num_params)\n",
    " \n",
    "# centers = torch.rand((5,8))\n",
    "# rbf_net = RBFN(centers)\n",
    "# rbf_net.print_network()\n",
    "# rbf_net.initialize_weights()\n",
    " \n",
    " \n",
    "if __name__ ==\"__main__\":\n",
    "    data = torch.tensor([[0.25, 0.75], [0.75,0.75], [0.25,0.5], [0.5,0.5],[0.75,0.5],\n",
    "                         [0.25,0.25],[0.75,0.25],[0.5,0.125],[0.75,0.125]], dtype=torch.float32)\n",
    "    label = torch.tensor([[-1,1,-1],[1,-1,-1],[-1,-1,1],[-1,-1,1],[-1,-1,1],\n",
    "                          [1,-1,-1],[-1,1,-1],[-1,1,-1],[1,-1,-1]], dtype=torch.float32)\n",
    "    print(data.size())\n",
    " \n",
    "    centers = data[0:8,:]\n",
    "    rbf = RBFN(centers,3)\n",
    "    # params = rbf.parameters()\n",
    "    # loss_fn = torch.nn.MSELoss()\n",
    "    # optimizer = torch.optim.SGD(params,lr=0.1,momentum=0.9)\n",
    " \n",
    "    # for i in range(10000):\n",
    "    #     optimizer.zero_grad()\n",
    " \n",
    "    #     y = rbf.forward(data)\n",
    "    #     loss = loss_fn(y,label)\n",
    "    #     loss.backward()\n",
    "    #     optimizer.step()\n",
    "    #     print(i,\"\\t\",loss.data)\n",
    " \n",
    "    # # 加载使用\n",
    "    # y = rbf.forward(data)\n",
    "    # print(y.data)\n",
    "    # print(label.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\Applications\\vscode\\workspace\\JMLM\\test.ipynb Cell 35\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Applications/vscode/workspace/JMLM/test.ipynb#X46sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m test \u001b[39m=\u001b[39m [[\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m]]\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Applications/vscode/workspace/JMLM/test.ipynb#X46sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m x1, x2, x3 \u001b[39min\u001b[39;00m test:\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Applications/vscode/workspace/JMLM/test.ipynb#X46sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39mprint\u001b[39m(x1)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Applications/vscode/workspace/JMLM/test.ipynb#X46sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39mprint\u001b[39m(x2)\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": [
    "test = [[1, 2]]\n",
    "\n",
    "for x1, x2 in test:\n",
    "    print(x1)\n",
    "    print(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "47b75fb8d8aa71c612710195ff4b16aa9497d5554f8641a297ce0b115b33ab61"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
